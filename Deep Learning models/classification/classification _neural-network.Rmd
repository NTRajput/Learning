---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

###Deep Learning Classification  Model 
####1.Data Prepration 
     Load the Dataset
```{r}
#Billed calls data
dataset2= read.csv('Invoiced December.csv')
#Non bill calls data 
data1 = read.csv('Non-bill.csv')
#Combine both files so that we can process both files 
#This step is imporatnat as in our Target variable we need both labels to train our data
b<-colnames(data1)
library (dplyr)
data_set=select(dataset2,b)
beta=rbind(data_set,data1)
colnames(beta)
```

###2.Data Encode :

####a)Convert the selected variables into Numeric 
```{r}
#remove to text columns before mutate
which( colnames(beta)=="Billing.Notes")
which( colnames(beta)=="Call.Text")
beta1=beta[,-c(6,5)]
library (dplyr)
library(magrittr)
beta1[,-1] %<>% mutate_if(is.factor,as.numeric)
```


#### b)Convert the Text Columns to Word Matrix   


####c)Bag of Words Model for-'Billing Notes' Variable - One column for each word 


    Preparing text for Bag of Words Model 
```{r}
library(tm)
corpus_a = VCorpus(VectorSource(beta$Billing.Notes))
#Cleaning the text before converting to Bag 
#a)Convert all words in lower case
corpus_a= tm_map(corpus_a, content_transformer(tolower))
#b)Convert all words in Plain text Document
corpus_a<- tm_map(corpus_a, PlainTextDocument)
#c)removing any  numbers from text
corpus_a= tm_map(corpus_a, removeNumbers)
#d)removing punctuations 
corpus_a= tm_map(corpus_a, removePunctuation)
#d)removing Stopwords
corpus_a= tm_map(corpus_a, removeWords,stopwords() )
#e)Stemming the document 
corpus_a= tm_map(corpus_a, stemDocument)
library(SnowballC)
#removing extra spaces = stripWhitespace
corpus_a= tm_map(corpus_a, stripWhitespace)


#Creating the bag of words model - One column for each word 
library(tm)
bag_a = DocumentTermMatrix(corpus_a)
#only considering Most Frequent Words for understanding correlation ,reducing sparsity 
#second input is the proportion of words that are repeated to be kept in the bag
library(tm)
bag_a= removeSparseTerms(bag_a, 0.99)
#sparse matrix into a dataframe
bag_a =(as.matrix(bag_a))
```


#####d).Bag of Words Model for 'Call Text' Variable 

```{r}
library(tm)
corpus_b = VCorpus(VectorSource(beta$Call.Text))
#all words in lower case
library(tm)
corpus_b= tm_map(corpus_b, content_transformer(tolower))
corpus_b<- tm_map(corpus_b, PlainTextDocument)
#removing the numbers from text
corpus_b= tm_map(corpus_b, removeNumbers)
#removing punctuations 
corpus_b= tm_map(corpus_b, removePunctuation)

library(SnowballC)
corpus_b = tm_map(corpus_b, removeWords,stopwords() )
corpus_b= tm_map(corpus_b, stemDocument)

#removing extra spaces = stripWhitespace
corpus_b= tm_map(corpus_b, stripWhitespace)

#Creating the bag of words model - One column for each word 
library(tm)
bag_b = DocumentTermMatrix(corpus_b)
#only considering Most Frequent Words for understanding correlation ,reducing sparsity 
#second input is the proportion of words that are repeated to be kept in the bag
bag_b= removeSparseTerms(bag_b, 0.99)
#sparse matrix into a dataframe
bag_b =(as.matrix(bag_b))
#Save the final processed matrix in one varaible 
data2=cbind(beta1,bag_a,bag_b)

#Write the file 
#write.csv(data2 , 'data2.csv')

```
####4.Processed data 
    
    Partition the Processed Dataset into
    Train , Test and Validation Set 
    Ratio for partition - 60% , 20% and 20% respectively .
```{r}
#loading the new processed file 
Data2<-read.csv('data2.csv')
str(Data2)
Data2$Invoiced..Y.N.=ifelse(Data2$Invoiced..Y.N.=='Y',1,0)
unique(Data2$Invoiced..Y.N.)
Data2<-Data2[,-1]
dim(Data2)
new2<-as.matrix(Data2)

#Data Partition 
library(caTools)
set.seed(2019)
ind<-sample(1:3,nrow(new2), replace = T, prob= c(0.7,0.15,0.15))
training <-new2[ind==1,2:504]
head(training)
trainingtarget<-new2[ind==1,1]
unique(trainingtarget)
test<-new2[ind==2,2:504]
testtarget<-new2[ind==2,1]
validation <-new2[ind==3,2:504]
validation.target<-new2[ind==3,1]

#Changing target Variables to Matrix form 
library(keras)
library(neuralnet)
library(magrittr)
library(tensorflow)
unique(testtarget)
traininglabels=to_categorical(trainingtarget)
testlabels=to_categorical(testtarget)
validationlabels=to_categorical(validation.target)
unique(traininglabels)
```
####4.Deep Learning Model  


##### Tuning hyperparameter and trying models 
#####Final Neural Network Model
```{r}
# Building the Artificial Neural Network
model_keras <- keras_model_sequential()

model_keras %>% 
  
  # first hidden layer
  layer_dense(
    units = 16, 
    kernel_initializer = "uniform", 
    activation = "relu", 
    input_shape = c(503)) %>% 
  
  # dropout to prevent overfitting
  layer_dropout(rate = 0.1) %>%
  
  # second hidden layer
  layer_dense(
    units = 16, 
    kernel_initializer = "uniform", 
    #Regularization to prevent overfitting 
    kernel_regularizer =regularizer_l1(0.05),
    activation  = "relu") %>% 
  
  # dropout to prevent overfitting
  layer_dropout(rate = 0.1) %>%
  
  # output layer
  layer_dense(
    units = 2, 
    kernel_initializer = "uniform", 
    activation = "softmax") %>% 
  
  # compile ANN
  compile(
    optimizer = 'adam',
    loss = 'binary_crossentropy',
    metrics = c('accuracy')
  )

model_keras

history<-model_keras %>% 
  fit(training,
      traininglabels,
      epochs=30,
      batch_size = 32,
      validation_split =0.4)
```

####5.Evaluate Model on Validation Set and finally on test set 

```{r}

###Evaluate on Validation Set - Tune Hyperparameters as per the results in this step 
model_keras %>% evaluate(validation, validationlabels)
pred<-model_keras %>% predict_classes(validation)
table(Predicted=pred, Actual=validation.target)
prob<-model_keras%>%predict_proba(validation)
###Evaluate on Test Set - Final Model Only 
model_keras %>% evaluate(test, testlabels)
pred1<-model_keras %>% predict_classes(test)
table(Predicted=pred1, Actual=testtarget)
prob2<-model_keras%>%predict_proba(test)
```
####Model Plot 
```{r}
plot(history)
```

####6. Model For Prediction On New Test Data Set .
   
   
    a)Processing the New Test file same as the Train set Processing 
```{r}
library(dplyr)
library(plyr)
library(tidyverse)
library(magrittr)
library(keras)
library(neuralnet)
columnmatch<-read.csv('data2.csv')
Test1<-read.csv('February_Test_Date_Corrected.csv')
#Matching the varaibles with the one selected in the Train set 
Test.File=Test1[,-c(1,3,11,13,16,17,23,24,26:29)]
which( colnames(Test.File)=="Billing.Notes")
which( colnames(Test.File)=="Call.Text")
Test_File=Test.File[,-c(4,5)]

##convert all other columns to Numeric
Test_File %<>% mutate_if(is.factor,as.numeric)

##Convert teh text columns to numeric 
#bag of words model 
#install.packages('tm')
library(tm)
corpus_c = VCorpus(VectorSource(Test.File$Billing.Notes))
#all words in lower case
library(tm)
corpus_c= tm_map(corpus_c, content_transformer(tolower))
library(tm)
corpus_c<- tm_map(corpus_c, PlainTextDocument)
#removing the numbers from text
corpus_c= tm_map(corpus_c, removeNumbers)
#removing punctuations 
corpus_c= tm_map(corpus_c, removePunctuation)
corpus_c= tm_map(corpus_c, removeWords,stopwords() )
corpus_c= tm_map(corpus_c, stemDocument)
#removing unnecesary / irrelevant words = stop words
#install.packages('SnowballC')
library(SnowballC)
#corpus = tm_map(corpus, stemDocument)
#removing extra spaces = stripWhitespace
corpus_c= tm_map(corpus_c, stripWhitespace)
#Creating the bag of words model - One column for each word 
library(tm)
bag_c= DocumentTermMatrix(corpus_c)
#only considering Most Frequent Words for understanding correlation ,reducing sparsity 
#second input is the proportion of words that are repeated to be kept in the bag
library(tm)
bag_c= removeSparseTerms(bag_c, 0.99)
#sparse matrix into a dataframe
bag_c =(as.matrix(bag_c))

##making the second bag for the call text...................... 
#process call text column
library(tm)
corpus_d = VCorpus(VectorSource(Test.File$Call.Text))
#all words in lower case
library(tm)
corpus_d= tm_map(corpus_d, content_transformer(tolower))
corpus_d<- tm_map(corpus_d, PlainTextDocument)
#removing the numbers from text
corpus_d= tm_map(corpus_d, removeNumbers)
#removing punctuations 
corpus_d= tm_map(corpus_d, removePunctuation)
#corpus_b=tm_map(corpus_b,removeURL)
#removing unnecesary / irrelevant words = stop words
#install.packages('SnowballC')
library(SnowballC)
corpus_d = tm_map(corpus_d, removeWords,stopwords() )
corpus_d= tm_map(corpus_d, stemDocument)

#removing extra spaces = stripWhitespace
corpus_d= tm_map(corpus_d, stripWhitespace)


#Creating the bag of words model - One column for each word 
library(tm)
bag_d = DocumentTermMatrix(corpus_d)
#only considering Most Frequent Words for understanding correlation ,reducing sparsity 
#second input is the proportion of words that are repeated to be kept in the bag
bag_d= removeSparseTerms(bag_d, 0.99)
#sparse matrix into a dataframe
bag_d =(as.matrix(bag_d))

final_test=cbind(Test_File,bag_c,bag_d)
#write.csv(final_test,'final_test.csv')

```
   
   
   b) Loading the Processed File to do the Final Testing and Making the Predictions 
```{r}
final_test<-read.csv('final_test.csv')
final_test<-final_test[,-1]
head(final_test)
dim(final_test)
final_test=as.matrix(final_test)
dim(final_test)
# ##Evaluate the Model on The test File for  final Prediction 
model_keras %>%evaluate(final_test)
pred3<-model_keras%>% predict_classes(final_test)
pred3
summary(pred3==1)

#Pie chart -Proportion Of Target Labels- Bills Vs.Non.Bill
```
     
     
     c)Plotting Final Results 
```{r}
#Pie chart -Proportion Of Target Labels- Bills Vs.Non.Bill
#(TOTAL CALLS IN Test File=136163 )
#  summary(pred3==1)
#       0         1
#       117037   19126 
# [1] 75.85614
# [1] 24.14386
# > 24.14+75.86
# [1] 100
a<-c(85.95 , 14.04)
calls<-c("Non-Billable Calls,85.95% ","Billable Calls,14.04%")
library(plotrix)
pie3D(a, labels =calls ,explode=0.3, main="Proportion of Billable Vs Non-Billable calls",col = c('Pink','light blue'),labelcex=1.5,labelcol='Black')
```
  
  
    c)Saving Final Prediction Results 
```{r}
Final_Prediction_OnTest_set<-pred3
#write.csv(Final_Prediction_OnTestset1,'Final_Prediction_OnTestset1.csv')
```

